{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a3ef25d",
   "metadata": {},
   "source": [
    "## 0. Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "027d869f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import SGD\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import pyreadr\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace55d0b",
   "metadata": {},
   "source": [
    "## 1. Implement a TARNet for predicting Conditional Average Treatment Effects (CATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "123649bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TarNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(TarNet, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim \n",
    "        \n",
    "        #Representation phi\n",
    "        self.phi = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, 256),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ELU()\n",
    "        )\n",
    "        \n",
    "        self.treatment = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        #Layer of factual and counter factural\n",
    "        self.y0 = nn.Sequential(\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(64, 1) \n",
    "        )\n",
    "        \n",
    "        self.y1 = nn.Sequential(\n",
    "            nn.Linear(256, 64), \n",
    "            nn.ELU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "          \n",
    "    def forward(self, inputs):\n",
    "        phi = self.phi(inputs)\n",
    "        \n",
    "        t_hat = self.treatment(inputs)\n",
    "        y0_hat = self.y0(phi)\n",
    "        y1_hat = self.y1(phi)\n",
    "                \n",
    "        out = torch.cat((y0_hat, y1_hat, t_hat), 1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac191c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFunc(y_true, t_true, predictions):\n",
    "    y = y_true.reshape(-1,1)\n",
    "    t = t_true.reshape(-1,1)\n",
    "    \n",
    "    y0_hat = predictions[:,0].reshape(-1,1)\n",
    "    y1_hat = predictions[:,1].reshape(-1,1)\n",
    "    \n",
    "    loss = torch.sum((1-t)*(y-y0_hat)**2) + torch.sum(t * (y-y1_hat)**2)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42c9433",
   "metadata": {},
   "source": [
    "## 2. Implement the IHDP dataset with response surface B as described in Bayesian Nonparametric Modeling for Causal Inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c333c50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def response_surface_B(t, X):\n",
    "    #Offset matrix\n",
    "    W = np.ones(X.shape)*0.5\n",
    "\n",
    "    #Regression vector\n",
    "    vals = [0, 0.1, 0.2, 0.3, 0.4]\n",
    "    probabilities = [0.6, 0.1, 0.1, 0.1, 0.1]\n",
    "    \n",
    "    np.random.seed(4)\n",
    "    beta_B = np.random.choice(vals, X.shape[1], p = probabilities)\n",
    "\n",
    "    mean_y0 = np.exp(np.dot((X+W), beta_B))\n",
    "    sigma_y0 = 1\n",
    "\n",
    "    mean_y1 = np.dot(X, beta_B)-15\n",
    "    sigma_y1 = 1\n",
    "\n",
    "    y0 = np.random.normal(mean_y0).reshape(-1, 1)\n",
    "    y1 = np.random.normal(mean_y1).reshape(-1, 1)\n",
    "    \n",
    "    y = (1-t)*y0 + t*y1\n",
    "    \n",
    "    return y, y0, y1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018ef0a2",
   "metadata": {},
   "source": [
    "## 3. Generate 10 random train / valid / and test splits of the IHDP dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04504906",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetch raw data from github\n",
    "res = requests.get('https://github.com/vdorie/npci/raw/master/examples/ihdp_sim/data/ihdp.RData')\n",
    "open('input_data.RData', 'wb').write(res.content)\n",
    "ihdp = pyreadr.read_r('input_data.RData')['ihdp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42e5d48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_sets(ihdp, seed, plot = False):\n",
    "    ihdpShuffled = ihdp.sample(frac = 1, random_state =  seed)\n",
    "\n",
    "    X = ihdpShuffled.loc[:, ihdpShuffled.columns != 'treat'].values\n",
    "\n",
    "    #Standardize X values\n",
    "    mu = np.mean(X, axis = 0)\n",
    "    sd = np.std(X, axis = 0)\n",
    "    X = ((X-mu)/sd)\n",
    "\n",
    "    t = ihdpShuffled.loc[:, ihdpShuffled.columns == 'treat'].values\n",
    "    y, y0, y1 = response_surface_B(t, X)\n",
    "\n",
    "    #train / valid / test splits: \n",
    "    N, D = X.shape\n",
    "    Ntrain = int(0.6 * N)\n",
    "    Nvalid = int(0.8 * N)\n",
    "\n",
    "    output = dict()\n",
    "    \n",
    "    output['X'] = [X[:Ntrain], X[Ntrain:Nvalid], X[Nvalid:]]\n",
    "    output['y'] = [y[:Ntrain], y[Ntrain:Nvalid], y[Nvalid:]]\n",
    "    output['t'] = [t[:Ntrain], t[Ntrain:Nvalid], t[Nvalid:]]\n",
    "    output['y0'] = [y0[:Ntrain], y0[Ntrain:Nvalid], y0[Nvalid:]]\n",
    "    output['y1'] = [y1[:Ntrain], y1[Ntrain:Nvalid], y1[Nvalid:]]\n",
    "    \n",
    "    if plot: \n",
    "        #plt.hist(y*t)\n",
    "        #plt.hist(y*(1-t))\n",
    "        sns.histplot(y[t==0], label = 'y | t = 0', color = 'blue')\n",
    "        sns.histplot(y[t==1], label = 'y | t = 1', color = 'green')\n",
    "        plt.legend()\n",
    "        print('Average for t=1: {}'.format(np.mean(t*y)))\n",
    "        print('Average for t=0: {}'.format(np.mean((1-t)*y)))\n",
    "        print('Average treatment effect: {}'.format(np.mean(t*y)- np.mean((1-t)*y)))\n",
    "        print('Average effect on treated (CATT): {}'.format(np.mean((y1-y0)*t)))\n",
    "    \n",
    "    return output, D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93a6c362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average for t=1: -5.714724562241749\n",
      "Average for t=0: 5.661309819229568\n",
      "Average treatment effect: -11.376034381471317\n",
      "Average effect on treated (CATT): -9.43245128799338\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "datasets, D = generate_data_sets(ihdp, 42, plot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfab94e",
   "metadata": {},
   "source": [
    "## 4. Train and tune one model on each of the 10 datasets you have created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a054e1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(inputs, treatments, outcomes, batchSize):\n",
    "    # loop over the dataset\n",
    "    for i in range(0, inputs.shape[0], batchSize):\n",
    "        # yield a tuple of the current batched data and labels\n",
    "        yield (torch.from_numpy(inputs[i:i + batchSize].astype(np.float32)), \n",
    "               torch.from_numpy(treatments[i:i + batchSize].astype(np.float32)),\n",
    "               torch.from_numpy(outcomes[i:i + batchSize].astype(np.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4853ebd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, datasets, EPOCHS = 10, BATCH_SIZE = 64, LR=1e-3, VERBOSE = False):\n",
    "    Xtrain, Xvalid, Xtest = datasets['X']\n",
    "    ytrain, yvalid, ytest = datasets['y']\n",
    "    ttrain, tvalid, ttest = datasets['t']\n",
    "    \n",
    "    opt = SGD(model.parameters(), lr=LR)\n",
    "    model_performance = []\n",
    "\n",
    "    # loop through the epochs\n",
    "    for epoch in range(0, EPOCHS):\n",
    "        # initialize tracker variables and set our model to trainable\n",
    "        if VERBOSE: print(\"[INFO] epoch: {}...\".format(epoch + 1))\n",
    "        trainLoss = 0\n",
    "        valLoss = 0\n",
    "        samples = 0\n",
    "        model.train()\n",
    "\n",
    "        # loop over the current batch of data\n",
    "        for (batchX, batcht, batchy) in next_batch(Xtrain, ttrain, ytrain, BATCH_SIZE):\n",
    "\n",
    "            # model, and calculate loss\n",
    "            predictions = model(batchX)\n",
    "            loss = lossFunc(batchy, batcht, predictions)\n",
    "\n",
    "            # zero the gradients accumulated from the previous steps,abs\n",
    "            # perform backpropagation, and update model parameters\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            # update training loss, accuracy, and the number of samples visited\n",
    "            trainLoss += loss.item()\n",
    "            samples += batchy.size(0)\n",
    "                \n",
    "        #Store model and validation error after each epoch: \n",
    "        PATH = \"model_checkpoints/model_state_epoch_\"+str(epoch)+\".pt\"\n",
    "        torch.save(model, PATH)\n",
    "    \n",
    "        predictions_valid = model(torch.from_numpy(Xvalid.astype(np.float32)))\n",
    "        valLoss = lossFunc(torch.from_numpy(yvalid.astype(np.float32)),\n",
    "                           torch.from_numpy(tvalid.astype(np.float32)),\n",
    "                           predictions_valid).detach().numpy()\n",
    "        model_performance.append(valLoss / yvalid.shape[0])\n",
    "\n",
    "        # display model progress on the training and validation data\n",
    "        trainTemplate = \"epoch: {} train loss: {:.3f} val loss {:.3f}\"\n",
    "        if VERBOSE: print(trainTemplate.format(epoch + 1, \n",
    "                                               (trainLoss / samples), \n",
    "                                               (valLoss / yvalid.shape[0])))\n",
    "            \n",
    "    #Load and return model with minimal validation error \n",
    "    best_epoch = np.argmin(model_performance)\n",
    "    print('Best epoch: {} Best val loss: {:.3f}'.format(best_epoch+1, model_performance[best_epoch]))\n",
    "    best_model_path = \"model_checkpoints/model_state_epoch_\"+str(best_epoch)+\".pt\"\n",
    "    best_model = torch.load(best_model_path)\n",
    "    best_model.eval()    \n",
    "    \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef6e919",
   "metadata": {},
   "source": [
    "## 5. Report the mean and standard error of the Root Mean Average Precision in Heterogeneous Effect (described in the above papers) across the 10 held out test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a06a8538",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PEHE(y0_true, y1_true, predictions):\n",
    "    y0_hat = predictions[:,0].reshape(-1,1)\n",
    "    y1_hat = predictions[:,1].reshape(-1,1)\n",
    "    return torch.mean(((y1_true - y0_true)- (y1_hat - y0_hat))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2614aad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Best epoch: 39 Best val loss: 1.804\n",
      "Iteration: 2\n",
      "Best epoch: 99 Best val loss: 1.540\n",
      "Iteration: 3\n",
      "Best epoch: 65 Best val loss: 1.144\n",
      "Iteration: 4\n",
      "Best epoch: 55 Best val loss: 1.489\n",
      "Iteration: 5\n",
      "Best epoch: 64 Best val loss: 1.340\n",
      "Iteration: 6\n",
      "Best epoch: 53 Best val loss: 1.653\n",
      "Iteration: 7\n",
      "Best epoch: 69 Best val loss: 1.661\n",
      "Iteration: 8\n",
      "Best epoch: 100 Best val loss: 2.855\n",
      "Iteration: 9\n",
      "Best epoch: 41 Best val loss: 1.405\n",
      "Iteration: 10\n",
      "Best epoch: 48 Best val loss: 1.492\n",
      "************************\n",
      "Mean of Root PEHE: 1.840\n",
      "Standard Error of Root PEHE: 0.117\n"
     ]
    }
   ],
   "source": [
    "RootPEHE = []\n",
    "for i in range(10):\n",
    "    print('Iteration: {}'.format(i+1))\n",
    "    datasets, D = generate_data_sets(ihdp, 42+i, plot = False)\n",
    "    \n",
    "    model = TarNet(D)\n",
    "    model = train_model(model, \n",
    "                        datasets, \n",
    "                        EPOCHS = 100, \n",
    "                        BATCH_SIZE = 64,\n",
    "                        LR = 1e-4,\n",
    "                        VERBOSE = False)\n",
    "\n",
    "    #Calcualte mean and standard error of the Root Mean Average Precision on test data     \n",
    "    _, _, Xtest = datasets['X']\n",
    "    _, _, ytest = datasets['y']\n",
    "    _, _, ttest = datasets['t']\n",
    "    _, _, y0test = datasets['y0']\n",
    "    _, _, y1test = datasets['y1']\n",
    "    \n",
    "    predictions_test = model(torch.from_numpy(Xtest.astype(np.float32)))\n",
    "    \n",
    "    RootPEHE.append(torch.sqrt(PEHE(torch.from_numpy(y0test.astype(np.float32)), \n",
    "                                    torch.from_numpy(y1test.astype(np.float32)), \n",
    "                                    predictions_test)).detach().numpy())\n",
    "    \n",
    "print('************************')\n",
    "print('Mean of Root PEHE: {:.3f}'.format(np.nanmean(RootPEHE)))\n",
    "print('Standard Error of Root PEHE: {:.3f}'.format(np.nanstd(RootPEHE)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tarnet2",
   "language": "python",
   "name": "tarnet2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
